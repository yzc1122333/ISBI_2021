{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "GMRSAet50++ final_4.047_20201103.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18a94cf5ff0a4937903061782aa625c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c6e555a193fd44d99f24804ab49a9888",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd983b0efaf74aa28c7b42c95019af0a",
              "IPY_MODEL_2ed7bd88598149da80ce10eff5f84235"
            ]
          }
        },
        "c6e555a193fd44d99f24804ab49a9888": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd983b0efaf74aa28c7b42c95019af0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e483c935271b436299857708610147cd",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a07da75f2a4479788793fea52e78080"
          }
        },
        "2ed7bd88598149da80ce10eff5f84235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1021840454d64ebca635a9009eb3e053",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:03&lt;00:00, 28.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b1c07a070a54c229d7361812becb6a3"
          }
        },
        "e483c935271b436299857708610147cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a07da75f2a4479788793fea52e78080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1021840454d64ebca635a9009eb3e053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b1c07a070a54c229d7361812becb6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yzc1122333/ISBI_2021/blob/main/GMRSAet50%2B%2B_final_4_047_20201103.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg2N16KFlrHZ",
        "outputId": "d7e81002-28a5-4492-a5cf-5786358ae4f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVm7g0sGPWl6"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_8OjcJMY61e",
        "outputId": "8a6f0a31-a955-46ff-e985-9672d1b8ee55"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version 20201102 --apt-packages libomp5 libopenblas-dev"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  5116  100  5116    0     0  18014      0 --:--:-- --:--:-- --:--:-- 18014\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20201102 ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |██████████████████████▊         | 40kB 10.3MB/s eta 0:00:01Uninstalling torch-1.7.0+cu101:\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (50.3.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "  Successfully uninstalled torch-1.7.0+cu101\n",
            "Uninstalling torchvision-0.8.1+cu101:\n",
            "  Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20201102-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][117.0 MiB/117.0 MiB]                                                \n",
            "Operation completed over 1 objects/117.0 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20201102-cp36-cp36m-linux_x86_64.whl...\n",
            "Done updating TPU runtime\n",
            "- [1 files][127.5 MiB/127.5 MiB]                                                \n",
            "Operation completed over 1 objects/127.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20201102-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  3.2 MiB/  3.2 MiB]                                                \n",
            "Operation completed over 1 objects/3.2 MiB.                                      \n",
            "Processing ./torch-nightly+20201102-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201102) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201102) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20201102) (3.7.4.3)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.8.0a0\n",
            "Processing ./torch_xla-nightly+20201102-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+2cd4f07\n",
            "Processing ./torchvision-nightly+20201102-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201102) (1.8.0a0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201102) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20201102) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20201102) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20201102) (3.7.4.3)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.9.0a0+f95b053\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenblas-dev is already the newest version (0.2.20+ds-4).\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (297 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJATr3IgUqWY",
        "outputId": "2b446d55-316f-4858-98a6-fcd3fe1b8d42"
      },
      "source": [
        "!pip install pretrainedmodels"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretrainedmodels\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/0e/be6a0e58447ac16c938799d49bfb5fb7a80ac35e137547fc6cee2c08c4cf/pretrainedmodels-0.7.4.tar.gz (58kB)\n",
            "\r\u001b[K     |█████▋                          | 10kB 15.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 20kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 51kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (1.8.0a0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (0.9.0a0+f95b053)\n",
            "Collecting munch\n",
            "  Downloading https://files.pythonhosted.org/packages/cc/ab/85d8da5c9a45e072301beb37ad7f833cd344e04c817d97e0cc75681d248f/munch-2.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pretrainedmodels) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch->pretrainedmodels) (0.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->pretrainedmodels) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from munch->pretrainedmodels) (1.15.0)\n",
            "Building wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-cp36-none-any.whl size=60964 sha256=e701f095acae34c12a82d34305190d6b1b8c77dc06fa854e68eb22926021dc20\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/df/63/62583c096289713f22db605aa2334de5b591d59861a02c2ecd\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: munch, pretrainedmodels\n",
            "Successfully installed munch-2.5.0 pretrainedmodels-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkxZXP1bvFKV",
        "outputId": "458b289a-12c1-4882-821a-bf66ceec3e90"
      },
      "source": [
        "!pip install albumentations==0.4.5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/40/a343ecacc7e22fe52ab9a16b84dc6165ba05ee17e3729adeb3e2ffa2b37b/albumentations-0.4.5.tar.gz (116kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.15.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.5)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n",
            "Building wheels for collected packages: albumentations, imgaug\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=64378 sha256=0fe559f9fa6597828005a96fd177796f5f4364a4e65af51fa360804fd2e5667a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/a0/61/e50f93165a5ec7e7f5d65064e513239505bc4c06d2289557d3\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654021 sha256=dadbf2dba82cb91a6627cdf9b3a28282879d5ff94062a5482807dc4bb3fb822c\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built albumentations imgaug\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRp7NhT6jVnG"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import _utils\n",
        "\n",
        "from random import choice\n",
        "\n",
        "from skimage import io\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import glob\n",
        "\n",
        "#from torchsummary import summary\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "# from apex import amp\n",
        "\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from albumentations.augmentations.transforms import Lambda, ShiftScaleRotate, HorizontalFlip, Normalize, RandomBrightnessContrast, RandomResizedCrop\n",
        "from albumentations.pytorch import ToTensor\n",
        "from albumentations import Compose, OneOrOther"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IhYIc93CvtbM",
        "outputId": "6155c7ac-7732-4119-aa48-8041db41b743"
      },
      "source": [
        "import albumentations\n",
        "albumentations.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0.4.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUJp0NcCW6sU",
        "outputId": "f905ffd5-aad7-4c9e-bcc0-c6347a4556fb"
      },
      "source": [
        "!cat /proc/cpuinfo | grep 'physical id' | sort | uniq | wc -l\n",
        "!cat /proc/cpuinfo |grep \"cores\"|uniq|awk '{print $4}'\n",
        "!grep 'processor' /proc/cpuinfo | sort -u | wc -l"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "20\n",
            "40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfzNYK4-ZKNL"
      },
      "source": [
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgZwl64lnnzk"
      },
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNz7YC-Sq_4x"
      },
      "source": [
        "train_df = pd.read_csv(f'/content/drive/My Drive/BAA/train.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzRygoRtwxkV",
        "outputId": "9d9a3cce-597a-41a3-fbe2-aa1d83408a7e"
      },
      "source": [
        "len(train_df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12611"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "nrIOGcpz6Fra",
        "outputId": "be0c303d-e920-45f0-e614-9dc0fb24b044"
      },
      "source": [
        "plt.title('Male')\n",
        "train_df['male'].astype(int).hist()\n",
        "plt.show()\n",
        "plt.title('Female_age')\n",
        "train_df[train_df['male']==False]['boneage'].hist()\n",
        "plt.show()\n",
        "plt.title('Male_age')\n",
        "train_df[train_df['male']==True]['boneage'].hist()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNWDSFBx9bJx"
      },
      "source": [
        "val_df = pd.read_csv(f'/content/drive/My Drive/BAA/Validation Dataset.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "H6LindwLBR3d",
        "outputId": "0d5afbfb-c91f-47d4-d5dc-94ffb5a209f5"
      },
      "source": [
        "plt.title('Male')\n",
        "val_df['male'].astype(int).hist()\n",
        "plt.show()\n",
        "plt.title('Female_age')\n",
        "val_df[train_df['male']==False]['boneage'].hist()\n",
        "plt.show()\n",
        "plt.title('Male_age')\n",
        "val_df[train_df['male']==True]['boneage'].hist()\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdX9onshZ3pk"
      },
      "source": [
        "boneage_mean = train_df['boneage'].mean()\n",
        "boneage_div = train_df['boneage'].std()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "cp1JpnvvgYS5",
        "outputId": "43bb6624-c28c-41f5-f81d-ff9bfcbf32ef"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>boneage</th>\n",
              "      <th>male</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1377</td>\n",
              "      <td>180</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1378</td>\n",
              "      <td>12</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1379</td>\n",
              "      <td>94</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1380</td>\n",
              "      <td>120</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1381</td>\n",
              "      <td>82</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12606</th>\n",
              "      <td>15605</td>\n",
              "      <td>50</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12607</th>\n",
              "      <td>15606</td>\n",
              "      <td>113</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12608</th>\n",
              "      <td>15608</td>\n",
              "      <td>55</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12609</th>\n",
              "      <td>15609</td>\n",
              "      <td>150</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12610</th>\n",
              "      <td>15610</td>\n",
              "      <td>132</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12611 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  boneage   male\n",
              "0       1377      180  False\n",
              "1       1378       12  False\n",
              "2       1379       94  False\n",
              "3       1380      120   True\n",
              "4       1381       82  False\n",
              "...      ...      ...    ...\n",
              "12606  15605       50  False\n",
              "12607  15606      113  False\n",
              "12608  15608       55  False\n",
              "12609  15609      150   True\n",
              "12610  15610      132   True\n",
              "\n",
              "[12611 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "wWZVe55hgawx",
        "outputId": "344d12e0-d811-4f6e-a7cb-20295c9dc2d4"
      },
      "source": [
        "val_df"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>male</th>\n",
              "      <th>boneage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1386</td>\n",
              "      <td>False</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1392</td>\n",
              "      <td>True</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1397</td>\n",
              "      <td>False</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1401</td>\n",
              "      <td>False</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1410</td>\n",
              "      <td>True</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1420</th>\n",
              "      <td>15592</td>\n",
              "      <td>False</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>15601</td>\n",
              "      <td>False</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>15607</td>\n",
              "      <td>True</td>\n",
              "      <td>186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>15611</td>\n",
              "      <td>False</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>15612</td>\n",
              "      <td>True</td>\n",
              "      <td>132</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1425 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id   male  boneage\n",
              "0      1386  False       30\n",
              "1      1392   True      162\n",
              "2      1397  False       18\n",
              "3      1401  False      132\n",
              "4      1410   True       57\n",
              "...     ...    ...      ...\n",
              "1420  15592  False       42\n",
              "1421  15601  False      132\n",
              "1422  15607   True      186\n",
              "1423  15611  False      120\n",
              "1424  15612   True      132\n",
              "\n",
              "[1425 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUavllAYP1LQ"
      },
      "source": [
        "test_df = pd.read_excel('/content/drive/My Drive/BAA/Bone age ground truth.xlsx')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlDFjLhZfhx-"
      },
      "source": [
        "### Create data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juo73f72XhZT"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRfD97-YWBrw"
      },
      "source": [
        "# norm_mean = [0.485, 0.456, 0.406]\n",
        "# norm_std = [0.229, 0.224, 0.225]\n",
        "# #R*0.299 + G*0.587 + B*0.114\n",
        "# a = np.array([0.299, 0.587, 0.114])\n",
        "# a.dot(np.array(norm_mean))\n",
        "# a.dot(np.array(norm_std))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B_59mX-7F0E"
      },
      "source": [
        "norm_mean = [0.143] #0.458971\n",
        "norm_std = [0.144] #0.225609\n",
        "\n",
        "# norm_mean = [0.143, 0.143, 0.143] \n",
        "# norm_std = [0.144, 0.144, 0.144] \n",
        "\n",
        "RandomErasing = transforms.RandomErasing(scale=(0.02, 0.08), ratio = (0.5, 2), p = 0.8)\n",
        "\n",
        "def randomErase(image, **kwargs):\n",
        "    return RandomErasing(image)\n",
        "\n",
        "def sample_normalize(image, **kwargs):\n",
        "    image = image/255\n",
        "    channel = image.shape[2]\n",
        "    mean, std = image.reshape((-1, channel)).mean(axis = 0), image.reshape((-1, channel)).std(axis = 0)\n",
        "    return (image-mean)/(std + 1e-3)\n",
        "\n",
        "\n",
        "# rotation range of 20 degrees, horizontal/vertical translation up to 20%, zoom up to 20% and a horizontal flip\n",
        "# transform_train = Compose([\n",
        "#     ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, p = 0.8),\n",
        "#     HorizontalFlip(p = 0.5),\n",
        "#     RandomBrightnessContrast(p = 0.8), \n",
        "#     Normalize(mean=norm_mean, std = norm_std, p = 1),                                    \n",
        "#     ToTensor(),\n",
        "#     Lambda(image = randomErase) \n",
        "    \n",
        "# ])\n",
        "\n",
        "# transform_val = Compose([                                   \n",
        "#     Normalize(mean=norm_mean, std = norm_std, p = 1),\n",
        "#     ToTensor(),\n",
        "# ])\n",
        "\n",
        "transform_train = Compose([\n",
        "    # RandomBrightnessContrast(p = 0.8),\n",
        "    RandomResizedCrop(512, 512, (0.5, 1.0), p = 0.5),\n",
        "    ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, border_mode = cv2.BORDER_CONSTANT, value = 0.0, p = 0.8),\n",
        "    # HorizontalFlip(p = 0.5),\n",
        "    \n",
        "    # ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, p = 0.8),\n",
        "    HorizontalFlip(p = 0.5),\n",
        "    RandomBrightnessContrast(p = 0.8, contrast_limit=(-0.3, 0.2)),                             \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "    Lambda(image = randomErase) \n",
        "    \n",
        "])\n",
        "\n",
        "transform_val = Compose([                                   \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = Compose([                                   \n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LE-OKIDiDOTk"
      },
      "source": [
        "# from torchvision.transforms import Normalize\n",
        "\n",
        "# class Unnormalize:\n",
        "#     \"\"\"Converts an image tensor that was previously Normalize'd\n",
        "#     back to an image with pixels in the range [0, 1].\"\"\"\n",
        "#     def __init__(self, mean, std):\n",
        "#         self.mean = mean\n",
        "#         self.std = std\n",
        "\n",
        "#     def __call__(self, tensor):\n",
        "#         mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device)\n",
        "#         std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device)\n",
        "#         return torch.clamp(tensor*std + mean, 0., 1.)\n",
        "\n",
        "# unnormalize_transform = Unnormalize(norm_mean, norm_std)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CEpURmHCIL"
      },
      "source": [
        "# def read_image(path, image_size = 512):\n",
        "#     img = Image.open(path)\n",
        "#     w, h = img.size\n",
        "#     long = max(w, h)\n",
        "#     w, h = int(w/long*image_size), int(h/long*image_size)\n",
        "#     img = img.resize((w, h), Image.ANTIALIAS)\n",
        "#     delta_w, delta_h = image_size - w, image_size - h\n",
        "#     padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "#     return np.expand_dims(np.array(ImageOps.expand(img, padding)), axis=2)\n",
        "\n",
        "def read_image(path, image_size = 512):\n",
        "    img = Image.open(path)\n",
        "    w, h = img.size\n",
        "    long = max(w, h)\n",
        "    w, h = int(w/long*image_size), int(h/long*image_size)\n",
        "    img = img.resize((w, h), Image.ANTIALIAS)\n",
        "    delta_w, delta_h = image_size - w, image_size - h\n",
        "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "    return np.array(ImageOps.expand(img, padding).convert(\"RGB\"))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJN1OP-aCkLr"
      },
      "source": [
        "class BAATrainDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #nomalize boneage distribution\n",
        "            df['zscore'] = df['boneage'].map(lambda x: (x - boneage_mean)/boneage_div )\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        num = int(row['id'])\n",
        "        return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "class BAAValDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_val(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "        \n",
        "class BAATestDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = (df['Sex'] == 'M').astype('float32')\n",
        "            df['boneage'] = df['Ground truth bone age (months)'].astype('float32')\n",
        "            df['id'] = df['Case ID'].astype('int32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        print(self.df.head())\n",
        "        self.file_path = file_path\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_test(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df) \n",
        "\n",
        "def create_data_loader(train_df, val_df, test_df, train_root, val_root, test_root):\n",
        "    return BAATrainDataset(train_df, train_root), BAAValDataset(val_df, val_root), BAATestDataset(test_df, test_root)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY9pq74GLBum",
        "outputId": "58cb2eab-25ae-48ac-80d2-8130b068253c"
      },
      "source": [
        "!ls '/content/drive/My Drive/BAA/boneage-training-dataset/1377.png'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '/content/drive/My Drive/BAA/boneage-training-dataset/1377.png': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA-p8KZcUM0M",
        "outputId": "abdb2096-7624-4d53-cf14-04dbc2fb1d6f"
      },
      "source": [
        "train_set, val_set, test_set = create_data_loader(train_df, val_df, test_df, '/content/drive/My Drive/BAA/boneage-training-dataset', '/content/drive/My Drive/BAA/boneage-validation-dataset', '/content/drive/My Drive/BAA/Test Set Images')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Case ID Sex  Ground truth bone age (months)  male     boneage    id\n",
            "0     4360   M                      168.934249   1.0  168.934250  4360\n",
            "1     4361   M                      169.652678   1.0  169.652679  4361\n",
            "2     4362   M                       73.256112   1.0   73.256111  4362\n",
            "3     4363   M                      152.862669   1.0  152.862671  4363\n",
            "4     4364   M                      135.456954   1.0  135.456955  4364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly2WDirlfo6P"
      },
      "source": [
        "### Define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "18a94cf5ff0a4937903061782aa625c9",
            "c6e555a193fd44d99f24804ab49a9888",
            "cd983b0efaf74aa28c7b42c95019af0a",
            "2ed7bd88598149da80ce10eff5f84235",
            "e483c935271b436299857708610147cd",
            "8a07da75f2a4479788793fea52e78080",
            "1021840454d64ebca635a9009eb3e053",
            "7b1c07a070a54c229d7361812becb6a3"
          ]
        },
        "id": "br4AbYctLaaN",
        "outputId": "381a9b86-6c4e-48b6-f004-015a94a4082c"
      },
      "source": [
        "from pretrainedmodels import se_resnext101_32x4d, se_resnet152, xception, inceptionv4, inceptionresnetv2\n",
        "from torchvision.models import resnet34, resnet50\n",
        "\n",
        "def get_My_resnet34():\n",
        "    model = resnet34(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet50():\n",
        "    model = resnet50(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnet152():\n",
        "    model = se_resnet152(pretrained = None)\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnext101_32x4d():\n",
        "    model = se_resnext101_32x4d()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionv4():\n",
        "    model = inceptionv4()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    \n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionresnetv2():\n",
        "    model = inceptionresnetv2()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    \n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "# get_My_se_resnet152()\n",
        "a, b = get_My_resnet50()\n",
        "a"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18a94cf5ff0a4937903061782aa625c9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
              " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
              " ReLU(inplace=True),\n",
              " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
              " Sequential(\n",
              "   (0): Bottleneck(\n",
              "     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): Bottleneck(\n",
              "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (2): Bottleneck(\n",
              "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): Bottleneck(\n",
              "     (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): Bottleneck(\n",
              "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (2): Bottleneck(\n",
              "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (3): Bottleneck(\n",
              "     (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): Bottleneck(\n",
              "     (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (2): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (3): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (4): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (5): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              " ),\n",
              " Sequential(\n",
              "   (0): Bottleneck(\n",
              "     (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "     (downsample): Sequential(\n",
              "       (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "       (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     )\n",
              "   )\n",
              "   (1): Bottleneck(\n",
              "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              "   (2): Bottleneck(\n",
              "     (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "     (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "     (relu): ReLU(inplace=True)\n",
              "   )\n",
              " )]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIGDLj8Rg86W"
      },
      "source": [
        "class Pooling_attention(nn.Module):\n",
        "  def __init__(self, input_channels, kernel_size = 1):\n",
        "    super(Pooling_attention, self).__init__()\n",
        "    self.pooling_attention = nn.Sequential(\n",
        "        nn.Conv2d(input_channels, 1, kernel_size = kernel_size, padding = kernel_size//2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.pooling_attention(x) \n",
        "\n",
        "\n",
        "# class Part_Relation(nn.Module):\n",
        "#   def __init__(self, input_channels, reduction = 16):\n",
        "#     super(Part_Relation, self).__init__()\n",
        "#     self.pooling_attention_0 = nn.Sequential(\n",
        "#       nn.Conv2d(input_channels, input_channels//reduction, kernel_size = 1),\n",
        "#       nn.BatchNorm2d(input_channels//reduction),\n",
        "#       nn.ReLU()\n",
        "#     )\n",
        "#     self.pooling_attention_1 = Pooling_attention(input_channels//reduction, 1)\n",
        "#     self.pooling_attention_3 = Pooling_attention(input_channels//reduction, 3)\n",
        "#     self.pooling_attention_5 = Pooling_attention(input_channels//reduction, 5)\n",
        "\n",
        "#     self.last_conv =  nn.Sequential(\n",
        "#         nn.Conv2d(3, 1, kernel_size = 1),\n",
        "#         nn.Sigmoid()\n",
        "#     )\n",
        "\n",
        "\n",
        "    # self.se = SEModule(input_channels, 16)\n",
        "#   def forward(self, x):\n",
        "#     input = x\n",
        "#     x = self.pooling_attention_0(x)\n",
        "#     x = torch.cat([self.pooling_attention_1(x), self.pooling_attention_3(x), self.pooling_attention_5(x)], dim = 1)\n",
        "#     x = self.last_conv(x)\n",
        "#     return input + input*x\n",
        "\n",
        "class Part_Relation(nn.Module):\n",
        "  def __init__(self, input_channels, reduction = [16], level = 1):\n",
        "    super(Part_Relation, self).__init__()\n",
        "    \n",
        "    modules = []\n",
        "    for i in range(level):\n",
        "        output_channels = input_channels//reduction[i]\n",
        "        modules.append(nn.Conv2d(input_channels, output_channels, kernel_size = 1))\n",
        "        modules.append(nn.BatchNorm2d(output_channels))\n",
        "        modules.append(nn.ReLU())\n",
        "        input_channels = output_channels\n",
        "\n",
        "    self.pooling_attention_0 = nn.Sequential(*modules)\n",
        "    self.pooling_attention_1 = Pooling_attention(input_channels, 1)\n",
        "    self.pooling_attention_3 = Pooling_attention(input_channels, 3)\n",
        "    self.pooling_attention_5 = Pooling_attention(input_channels, 5)\n",
        "\n",
        "    self.last_conv =  nn.Sequential(\n",
        "        nn.Conv2d(3, 1, kernel_size = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    input = x\n",
        "    x = self.pooling_attention_0(x)\n",
        "    x = torch.cat([self.pooling_attention_1(x), self.pooling_attention_3(x), self.pooling_attention_5(x)], dim = 1)\n",
        "    x = self.last_conv(x)\n",
        "    return input - input*x"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nupnBC96r54k"
      },
      "source": [
        "# class BAA_New(nn.Module):\n",
        "#     def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "#         super(BAA_New, self).__init__()\n",
        "#         self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "#         self.out_channels = out_channels\n",
        "#         self.backbone1 = backbone[5]\n",
        "#         self.backbone2 = backbone[6]\n",
        "#         self.backbone3 = backbone[7]\n",
        "\n",
        "#         #3.788\n",
        "#         # self.part_relation0 = Part_Relation(256)\n",
        "#         # self.part_relation1 = Part_Relation(512, 32)\n",
        "#         # self.part_relation2 = Part_Relation(1024, 8, 2)\n",
        "#         # self.part_relation3 = Part_Relation(2048, 8, 2)\n",
        "\n",
        "#         self.part_relation0 = Part_Relation(256)\n",
        "#         self.part_relation1 = Part_Relation(512, [4, 8], 2)\n",
        "#         self.part_relation2 = Part_Relation(1024, [8, 8], 2)\n",
        "#         self.part_relation3 = Part_Relation(2048, [8, 16], 2)\n",
        "\n",
        "#         self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "#         self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "#         self.fc0 = nn.Linear(out_channels + gender_encode_length, 1024)\n",
        "#         self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "#         self.fc1 = nn.Linear(1024, 512)\n",
        "#         self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "#         self.output = nn.Linear(512, 1)\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         x = self.part_relation0(self.backbone0(image))\n",
        "#         # x  = self.backbone0(image)\n",
        "#         x = self.part_relation1(self.backbone1(x))\n",
        "#         # x = self.backbone1(x)\n",
        "#         x = self.part_relation2(self.backbone2(x))\n",
        "#         # x = self.backbone2(x)\n",
        "#         x = self.part_relation3(self.backbone3(x))\n",
        "#         # x = self.backbone3(x)\n",
        "\n",
        "#         x = F.adaptive_avg_pool2d(x, 1)\n",
        "#         x = torch.squeeze(x)\n",
        "#         x.view(-1, self.out_channels)\n",
        "\n",
        "#         gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "#         gender_encode = F.relu(gender_encode)\n",
        "\n",
        "#         x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "#         x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "#         x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "#         return self.output(x)\n",
        "\n",
        "class BAA_New(nn.Module):\n",
        "    def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "        super(BAA_New, self).__init__()\n",
        "        self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "        self.part_relation0 = Part_Relation(256)\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone1 = backbone[5]\n",
        "        self.part_relation1 = Part_Relation(512, [4, 8], 2)\n",
        "        self.backbone2 = backbone[6]\n",
        "        self.part_relation2 = Part_Relation(1024, [8, 8], 2)\n",
        "        self.backbone3 = backbone[7]\n",
        "        self.part_relation3 = Part_Relation(2048, [8, 16], 2)\n",
        "\n",
        "        #3.788\n",
        "        # self.part_relation0 = Part_Relation(256)\n",
        "        # self.part_relation1 = Part_Relation(512, 32)\n",
        "        # self.part_relation2 = Part_Relation(1024, 8, 2)\n",
        "        # self.part_relation3 = Part_Relation(2048, 8, 2)\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "        self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "        self.fc0 = nn.Linear(out_channels + gender_encode_length, 1024)\n",
        "        self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.output = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        x = self.part_relation0(self.backbone0(image))\n",
        "        # x  = self.backbone0(image)\n",
        "        x = self.part_relation1(self.backbone1(x))\n",
        "        # x = self.backbone1(x)\n",
        "        x = self.part_relation2(self.backbone2(x))\n",
        "        # x = self.backbone2(x)\n",
        "        x = self.part_relation3(self.backbone3(x))\n",
        "        # x = self.backbone3(x)\n",
        "        feature_map = x\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = torch.squeeze(x)\n",
        "        x = x.view(-1, self.out_channels)\n",
        "\n",
        "        gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "        gender_encode = F.relu(gender_encode)\n",
        "\n",
        "        x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        x = self.output(x)\n",
        "\n",
        "        return feature_map, gender_encode"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CHdKWC57YQV",
        "outputId": "7cdce5eb-e685-4335-f054-53c8b731e072"
      },
      "source": [
        "model = BAA_New(32, *get_My_resnet50())\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/BAA/MRSA_50++_4.03/best_MRSA_50++_4.03.bin'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-do7TWGt2Dr5",
        "outputId": "c9f86cc4-56ce-4971-830f-09393b992275"
      },
      "source": [
        "class Self_Attention_Adj(nn.Module):\n",
        "    def __init__(self, feature_size, attention_size):\n",
        "        super(Self_Attention_Adj, self).__init__()\n",
        "        self.queue = nn.Parameter(torch.empty(feature_size, attention_size))\n",
        "        nn.init.kaiming_uniform_(self.queue)\n",
        "\n",
        "        self.key = nn.Parameter(torch.empty(feature_size, attention_size))\n",
        "        nn.init.kaiming_uniform_(self.key)\n",
        "\n",
        "        self.leak_relu = nn.LeakyReLU()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        Q = self.leak_relu(torch.matmul(x, self.queue))\n",
        "        K = self.leak_relu(torch.matmul(x, self.key))\n",
        "\n",
        "        return self.softmax(torch.matmul(Q, K.transpose(1, 2)))\n",
        "\n",
        "\n",
        "\n",
        "class Graph_GCN(nn.Module):\n",
        "    def __init__(self, node_size, feature_size, output_size):\n",
        "        super(Graph_GCN, self).__init__()\n",
        "        self.node_size = node_size\n",
        "        self.feature_size = feature_size\n",
        "        self.output_size = output_size\n",
        "        self.weight = nn.Parameter(torch.empty(feature_size, output_size))\n",
        "        nn.init.kaiming_uniform_(self.weight)\n",
        "        \n",
        "    def forward(self, x, A):\n",
        "        x = torch.matmul(A, x.transpose(1, 2))\n",
        "        return (torch.matmul(x, self.weight)).transpose(1, 2)\n",
        "\n",
        "# class Graph_BAA(nn.Module):\n",
        "#     def __init__(self, image_backbone, k = 20):\n",
        "#         self.image_backbone = image_backbone\n",
        "#         self.gconv = Graph_GCN(16*16, 2048 + 1, 1024)\n",
        "#         self.fc = nn.Linear(1024, 1)\n",
        "#         # self.top_k = k\n",
        "\n",
        "#     def forward(self, image, gender):\n",
        "#         #input image to backbone, 16*16*2048\n",
        "#         feature_map = self.image_backbone(iamge).view(-1,16*16,2048)\n",
        "#         #calculate the score the select top 20 points from 16*16\n",
        "#         # using an parameter free spatial attention to get a score\n",
        "#         score = F.adaptive_avg_pool1d(feature_map)\n",
        "#         node_feature = feature_map[:,torch.argsort(score, dim = 1)[: self.top_k],:]\n",
        "\n",
        "#         #convert gender to a node, 1x1->1x2048, padding zeros\n",
        "#         gender = gender.expand(-1, 2048).view(-1, 1, 2048)\n",
        "#         #concatenate the image and gender node\n",
        "#         node_feature = torch.concat([node_feature, gender], dim = 1)\n",
        "#         x = F.adaptive_avg_pool1d(self.gconv(node_feature), 1)\n",
        "\n",
        "#         return self.fc(x)\n",
        "\n",
        "class Graph_BAA(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super(Graph_BAA, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        #freeze image backbone\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.adj_learning = Self_Attention_Adj(2048, 256)\n",
        "        self.gconv = Graph_GCN(16*16, 2048, 1024)\n",
        "\n",
        "        self.fc0 = nn.Linear(1024 + 32, 1024)\n",
        "        self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # self.fc1 = nn.Linear(1024, 512)\n",
        "        # self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.output = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        #input image to backbone, 16*16*2048\n",
        "        feature_map, gender = self.backbone(image, gender)\n",
        "        node_feature = feature_map.view(-1, 2048, 16*16)\n",
        "        A = self.adj_learning(node_feature)\n",
        "        x = F.leaky_relu(self.gconv(node_feature, A))\n",
        "        x = torch.squeeze(F.adaptive_avg_pool1d(x, 1))\n",
        "        x = torch.cat([x, gender], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "        # x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        return self.output(x)\n",
        "\n",
        "    def fine_tune(self, need_fine_tune = True):\n",
        "        self.train(need_fine_tune)\n",
        "        self.backbone.eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Graph_BAA(model)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Graph_BAA(\n",
              "  (backbone): BAA_New(\n",
              "    (backbone0): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (part_relation0): Part_Relation(\n",
              "      (pooling_attention_0): Sequential(\n",
              "        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "      )\n",
              "      (pooling_attention_1): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_3): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_5): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (last_conv): Sequential(\n",
              "        (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (backbone1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (part_relation1): Part_Relation(\n",
              "      (pooling_attention_0): Sequential(\n",
              "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (pooling_attention_1): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_3): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_5): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (last_conv): Sequential(\n",
              "        (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (backbone2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (part_relation2): Part_Relation(\n",
              "      (pooling_attention_0): Sequential(\n",
              "        (0): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (pooling_attention_1): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_3): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_5): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (last_conv): Sequential(\n",
              "        (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (backbone3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (part_relation3): Part_Relation(\n",
              "      (pooling_attention_0): Sequential(\n",
              "        (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "      (pooling_attention_1): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_3): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (pooling_attention_5): Pooling_attention(\n",
              "        (pooling_attention): Sequential(\n",
              "          (0): Conv2d(16, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "          (1): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (last_conv): Sequential(\n",
              "        (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Sigmoid()\n",
              "      )\n",
              "    )\n",
              "    (gender_encoder): Linear(in_features=1, out_features=32, bias=True)\n",
              "    (gender_bn): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (fc0): Linear(in_features=2080, out_features=1024, bias=True)\n",
              "    (bn0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (output): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              "  (adj_learning): Self_Attention_Adj(\n",
              "    (leak_relu): LeakyReLU(negative_slope=0.01)\n",
              "    (softmax): Softmax(dim=1)\n",
              "  )\n",
              "  (gconv): Graph_GCN()\n",
              "  (fc0): Linear(in_features=1056, out_features=1024, bias=True)\n",
              "  (bn0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (output): Linear(in_features=1024, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSkhLpm_i7O"
      },
      "source": [
        "# def train_fn(net, train_loader, loss_fn, epoch, optimizer, device):\n",
        "#     '''\n",
        "#     checkpoint is a dict\n",
        "#     '''\n",
        "#     global total_size \n",
        "#     global training_loss \n",
        "\n",
        "#     batch_accumulator = 2\n",
        "#     backprop_flag = False\n",
        "#     net.train()\n",
        "#     if xm.is_master_ordinal():\n",
        "#         train_pbar = tqdm(train_loader)\n",
        "#         train_pbar.desc = f'Epoch {epoch + 1}'\n",
        "#     else:\n",
        "#         train_pbar = train_loader\n",
        "\n",
        "#     optimizer.zero_grad()\n",
        "#     for batch_idx, data in enumerate(train_pbar):\n",
        "#         # #put data to GPU\n",
        "#         backprop_flag = False\n",
        "#         size = len(data[1])\n",
        "        \n",
        "#         image, gender = data[0]\n",
        "#         image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "#         label = data[1].to(device)\n",
        "\n",
        "#         batch_size = len(data[1])\n",
        "#         label = data[1].to(device)\n",
        "        \n",
        "#         #forward\n",
        "#         y_pred = net(image, gender)\n",
        "#         y_pred = y_pred.squeeze()\n",
        "\n",
        "#         # print(y_pred, label)\n",
        "#         loss = loss_fn(y_pred, label)\n",
        "#         #backward,calculate gradients\n",
        "#         loss.backward()\n",
        "\n",
        "#         if batch_idx%batch_accumulator == 1:\n",
        "#             backprop_flag = True\n",
        "            \n",
        "#             #backward,update parameter\n",
        "#             xm.optimizer_step(optimizer)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "            \n",
        "#         else:\n",
        "#             xm.rendezvous('123')\n",
        "\n",
        "#         #the learning rate should be update after optimizer's update \n",
        "#         #change the learning rate, because using One cycle pollicy,the learning rate should be update per mini-batch\n",
        "#         # scheduler.step()\n",
        "\n",
        "#         batch_loss = loss.item()\n",
        "\n",
        "#         training_loss += batch_loss\n",
        "#         total_size += batch_size\n",
        "#         if xm.is_master_ordinal():\n",
        "#             train_pbar.set_postfix({'loss': batch_loss/batch_size})\n",
        "#     else:\n",
        "#         if backprop_flag == False:\n",
        "#             #backward,update parameter\n",
        "#             xm.optimizer_step(optimizer)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "\n",
        "\n",
        "#     return training_loss/total_size "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNMQTCcURnKu"
      },
      "source": [
        "def train_fn(net, train_loader, loss_fn, epoch, optimizer, device):\n",
        "    '''\n",
        "    checkpoint is a dict\n",
        "    '''\n",
        "    global total_size \n",
        "    global training_loss \n",
        "\n",
        "    net.fine_tune()\n",
        "    if xm.is_master_ordinal():\n",
        "        train_pbar = tqdm(train_loader)\n",
        "        train_pbar.desc = f'Epoch {epoch + 1}'\n",
        "    else:\n",
        "        train_pbar = train_loader\n",
        "    for batch_idx, data in enumerate(train_pbar):\n",
        "        # #put data to GPU\n",
        "        size = len(data[1])\n",
        "        \n",
        "        image, gender = data[0]\n",
        "        image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "        label = data[1].to(device)\n",
        "\n",
        "        batch_size = len(data[1])\n",
        "        label = data[1].to(device)\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forward\n",
        "        y_pred = net(image, gender)\n",
        "        y_pred = y_pred.squeeze()\n",
        "\n",
        "        # print(y_pred, label)\n",
        "        loss = loss_fn(y_pred, label)\n",
        "        #backward,calculate gradients\n",
        "        loss.backward()\n",
        "        #backward,update parameter\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        #the learning rate should be update after optimizer's update \n",
        "        #change the learning rate, because using One cycle pollicy,the learning rate should be update per mini-batch\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "\n",
        "        training_loss += batch_loss\n",
        "        total_size += batch_size\n",
        "        if xm.is_master_ordinal():\n",
        "            train_pbar.set_postfix({'loss': batch_loss/batch_size})\n",
        "        # print('loss:', batch_loss/batch_size)\n",
        "        # print(f'xla:{xm.get_ordinal()}, batch is{batch_idx}, loss is {mse_loss/total_size}, {size}')\n",
        "    return training_loss/total_size "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5M8QF3ahu9K"
      },
      "source": [
        "def evaluate_fn(net, val_loader, device):\n",
        "    net.fine_tune(False)\n",
        "    \n",
        "    global mae_loss \n",
        "    global val_total_size \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(val_loader):\n",
        "            val_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            mae_loss += batch_loss\n",
        "    return mae_loss\n",
        "\n",
        "def test_fn(net, test_loader, device):\n",
        "    net.train(False)\n",
        "    \n",
        "    global test_mae_loss \n",
        "    global test_total_size \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            test_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            test_mae_loss += batch_loss\n",
        "    return mae_loss"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obFL4BVuhLpw"
      },
      "source": [
        "def reduce_fn(vals):\n",
        "    return sum(vals)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t81bdBGDPRB"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "\n",
        "def map_fn(index, flags):\n",
        "\n",
        "  ## Setup\n",
        "  root = '/content/drive/My Drive/BAA'\n",
        "  model_name = 'GMRSA50' \n",
        "  path = f'{root}/{model_name}'\n",
        "\n",
        "  if xm.is_master_ordinal():\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "        \n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  seed_everything(seed=flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()\n",
        "\n",
        "\n",
        "#   mymodel = BAA_base(32)\n",
        "  net = Graph_BAA(model)\n",
        "#   mymodel.load_state_dict(torch.load('/content/drive/My Drive/BAA/resnet50_pr_2/best_resnet50_pr_2.bin'))\n",
        "  net = net.to(device)\n",
        "  \n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "  \n",
        "  val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    val_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "  \n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=val_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "  \n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])  \n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  # Creates AlexNet for 10 classes\n",
        "  # Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "  net.fine_tune()\n",
        "\n",
        "  global best_loss \n",
        "  best_loss = float('inf')\n",
        "#   loss_fn =  nn.MSELoss(reduction = 'sum')\n",
        "  loss_fn = nn.L1Loss(reduction = 'sum')\n",
        "  lr = flags['lr']\n",
        "\n",
        "  wd = 0\n",
        "    \n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay=wd)\n",
        "#   optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = wd)\n",
        "  scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    global training_loss \n",
        "    training_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global total_size \n",
        "    total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global mae_loss \n",
        "    mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global val_total_size \n",
        "    val_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global test_mae_loss \n",
        "    test_mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global test_total_size \n",
        "    test_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "    # xm.rendezvous(\"initialization\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    train_fn(net, para_train_loader, loss_fn, epoch, optimizer, device)\n",
        "    \n",
        "    ## Evaluation\n",
        "    # Sets net to eval and no grad context\n",
        "    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
        "    evaluate_fn(net, para_val_loader, device)\n",
        "\n",
        "    para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "    test_fn(net, para_test_loader, device)\n",
        "\n",
        "    scheduler.step()\n",
        "    \n",
        "    xm.save(net.state_dict(), '/'.join([path, f'{model_name}.bin']))  \n",
        "    training_loss = xm.mesh_reduce('training_loss',training_loss,reduce_fn)\n",
        "    total_size = xm.mesh_reduce('total_size_reduce',total_size,reduce_fn)\n",
        "    mae_loss = xm.mesh_reduce('mae_loss_reduce',mae_loss,reduce_fn)\n",
        "    val_total_size = xm.mesh_reduce('val_total_size_reduce',val_total_size,reduce_fn)\n",
        "    test_mae_loss = xm.mesh_reduce('test_mae_loss_reduce',test_mae_loss,reduce_fn)\n",
        "    test_total_size = xm.mesh_reduce('test_total_size_reduce',test_total_size,reduce_fn)\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(test_total_size)\n",
        "        train_loss, val_mae, test_mae = training_loss/total_size, mae_loss/val_total_size, test_mae_loss/test_total_size\n",
        "        print(f'training loss is {train_loss}, val loss is {val_mae}, test loss is {test_mae}, time : {time.time() - start_time}, lr:{optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal() and best_loss >= test_mae:\n",
        "        best_loss = test_mae\n",
        "        shutil.copy(f'{path}/{model_name}.bin', \\\n",
        "                    f'{path}/best_{model_name}.bin')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMOUJmb1mnag",
        "outputId": "05809df8-5bd8-4782-c213-1b75c74a51e7"
      },
      "source": [
        "flags = {}\n",
        "flags['lr'] = 5e-4\n",
        "flags['batch_size'] = 32\n",
        "flags['num_workers'] = 2\n",
        "flags['num_epochs'] = 100\n",
        "flags['seed'] = 1234\n",
        "\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 49/49 [07:54<00:00,  9.68s/it, loss=0.165]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.3873]), val loss is tensor([15.8779]), test loss is tensor([13.0494]), time : 609.6314787864685, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.162]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2355]), val loss is tensor([7.7334]), test loss is tensor([5.4884]), time : 401.0135827064514, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.179]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2284]), val loss is tensor([7.5471]), test loss is tensor([5.3705]), time : 400.247784614563, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.17]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2157]), val loss is tensor([8.3794]), test loss is tensor([5.3963]), time : 399.3979070186615, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.153]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2151]), val loss is tensor([7.9894]), test loss is tensor([7.0506]), time : 399.973646402359, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.208]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2144]), val loss is tensor([8.4459]), test loss is tensor([6.0086]), time : 401.5007815361023, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.161]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2131]), val loss is tensor([6.9314]), test loss is tensor([4.5053]), time : 400.1263151168823, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.151]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2051]), val loss is tensor([7.4950]), test loss is tensor([6.1522]), time : 399.4718499183655, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|██████████| 49/49 [05:47<00:00,  7.09s/it, loss=0.181]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2017]), val loss is tensor([7.6478]), test loss is tensor([6.1736]), time : 404.7007157802582, lr:0.0005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|██████████| 49/49 [05:47<00:00,  7.09s/it, loss=0.156]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.2044]), val loss is tensor([7.5168]), test loss is tensor([4.8073]), time : 403.33593559265137, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|██████████| 49/49 [05:48<00:00,  7.12s/it, loss=0.173]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1952]), val loss is tensor([6.6147]), test loss is tensor([4.8046]), time : 404.2060749530792, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|██████████| 49/49 [05:48<00:00,  7.12s/it, loss=0.171]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1911]), val loss is tensor([6.9607]), test loss is tensor([5.1149]), time : 405.03947472572327, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|██████████| 49/49 [05:49<00:00,  7.13s/it, loss=0.159]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1905]), val loss is tensor([6.8576]), test loss is tensor([5.3600]), time : 405.76618361473083, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|██████████| 49/49 [05:48<00:00,  7.11s/it, loss=0.174]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1910]), val loss is tensor([6.8365]), test loss is tensor([4.7159]), time : 403.6927800178528, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|██████████| 49/49 [05:50<00:00,  7.16s/it, loss=0.167]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1895]), val loss is tensor([6.9319]), test loss is tensor([5.0615]), time : 404.8666248321533, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.159]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1897]), val loss is tensor([6.9059]), test loss is tensor([4.3831]), time : 402.816689491272, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.153]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1853]), val loss is tensor([6.7581]), test loss is tensor([4.9101]), time : 399.5492777824402, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.186]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1865]), val loss is tensor([6.6704]), test loss is tensor([5.2586]), time : 401.17466855049133, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.151]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1855]), val loss is tensor([6.7707]), test loss is tensor([5.4887]), time : 398.6513066291809, lr:0.00025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 20: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.184]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1876]), val loss is tensor([8.6736]), test loss is tensor([5.9115]), time : 399.3262982368469, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 21: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.164]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1810]), val loss is tensor([6.4346]), test loss is tensor([4.5640]), time : 400.52365708351135, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 22: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.148]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1805]), val loss is tensor([6.6505]), test loss is tensor([4.3695]), time : 399.8444445133209, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 23: 100%|██████████| 49/49 [05:43<00:00,  7.00s/it, loss=0.15]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1790]), val loss is tensor([7.0814]), test loss is tensor([5.6047]), time : 400.21432757377625, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 24: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.164]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1805]), val loss is tensor([6.8378]), test loss is tensor([4.8697]), time : 400.61066818237305, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 25: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1787]), val loss is tensor([7.0004]), test loss is tensor([4.6869]), time : 400.45540475845337, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 26: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.167]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1807]), val loss is tensor([6.6205]), test loss is tensor([4.6478]), time : 402.78989481925964, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 27: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.138]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1785]), val loss is tensor([7.0603]), test loss is tensor([4.5519]), time : 400.83255791664124, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 28: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.155]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1786]), val loss is tensor([7.0620]), test loss is tensor([5.4769]), time : 400.2486035823822, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 29: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1769]), val loss is tensor([7.0352]), test loss is tensor([5.4526]), time : 401.35940051078796, lr:0.000125\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 30: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.148]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1752]), val loss is tensor([6.6171]), test loss is tensor([4.8978]), time : 400.39025139808655, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 31: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.165]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1739]), val loss is tensor([6.7359]), test loss is tensor([4.6056]), time : 401.6090519428253, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 32: 100%|██████████| 49/49 [05:45<00:00,  7.04s/it, loss=0.149]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1749]), val loss is tensor([6.8624]), test loss is tensor([4.7843]), time : 401.7999777793884, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 33: 100%|██████████| 49/49 [05:43<00:00,  7.02s/it, loss=0.149]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1749]), val loss is tensor([6.8074]), test loss is tensor([4.9278]), time : 398.6352336406708, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 34: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.162]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1721]), val loss is tensor([7.0789]), test loss is tensor([5.3691]), time : 401.68238735198975, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 35: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.142]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1749]), val loss is tensor([6.8413]), test loss is tensor([4.9498]), time : 400.8501513004303, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 36: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.154]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1739]), val loss is tensor([7.1456]), test loss is tensor([5.2871]), time : 400.2368381023407, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 37: 100%|██████████| 49/49 [05:46<00:00,  7.07s/it, loss=0.159]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1749]), val loss is tensor([6.5151]), test loss is tensor([4.8541]), time : 402.0499861240387, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 38: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.145]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1726]), val loss is tensor([6.9168]), test loss is tensor([4.8062]), time : 400.58694863319397, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 39: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.16]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1713]), val loss is tensor([6.7328]), test loss is tensor([4.9885]), time : 399.931396484375, lr:6.25e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 40: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1718]), val loss is tensor([7.0049]), test loss is tensor([5.1982]), time : 401.50863766670227, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 41: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1704]), val loss is tensor([6.3943]), test loss is tensor([4.3239]), time : 402.12146854400635, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 42: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1694]), val loss is tensor([6.8609]), test loss is tensor([4.6975]), time : 398.73919558525085, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 43: 100%|██████████| 49/49 [05:43<00:00,  7.02s/it, loss=0.149]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1695]), val loss is tensor([6.4299]), test loss is tensor([4.3528]), time : 400.033652305603, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 44: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1714]), val loss is tensor([6.3318]), test loss is tensor([4.1942]), time : 400.98389768600464, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 45: 100%|██████████| 49/49 [05:42<00:00,  7.00s/it, loss=0.135]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1704]), val loss is tensor([6.5649]), test loss is tensor([4.4917]), time : 397.9695599079132, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 46: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.133]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1688]), val loss is tensor([6.3159]), test loss is tensor([4.2557]), time : 400.4574360847473, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 47: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.145]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1685]), val loss is tensor([6.6784]), test loss is tensor([4.4734]), time : 401.7141671180725, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 48: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1696]), val loss is tensor([6.5290]), test loss is tensor([4.2850]), time : 400.5550172328949, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 49: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.151]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1702]), val loss is tensor([6.4056]), test loss is tensor([4.4444]), time : 398.9069230556488, lr:3.125e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 50: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.141]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1689]), val loss is tensor([6.3640]), test loss is tensor([4.2543]), time : 400.95169281959534, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 51: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.138]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1688]), val loss is tensor([6.4182]), test loss is tensor([4.1044]), time : 402.22457575798035, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 52: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.136]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1700]), val loss is tensor([6.3133]), test loss is tensor([4.3291]), time : 399.4108180999756, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 53: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1686]), val loss is tensor([6.3362]), test loss is tensor([4.1779]), time : 400.8910984992981, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 54: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1693]), val loss is tensor([6.3714]), test loss is tensor([4.1971]), time : 399.7308819293976, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 55: 100%|██████████| 49/49 [05:45<00:00,  7.04s/it, loss=0.132]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1679]), val loss is tensor([6.3032]), test loss is tensor([4.3121]), time : 401.0419578552246, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 56: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.128]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1683]), val loss is tensor([6.3462]), test loss is tensor([4.1816]), time : 400.50883197784424, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 57: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.151]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1675]), val loss is tensor([6.4031]), test loss is tensor([4.1202]), time : 400.98950242996216, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 58: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.146]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1679]), val loss is tensor([6.3606]), test loss is tensor([4.2334]), time : 399.58694529533386, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 59: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1669]), val loss is tensor([6.4158]), test loss is tensor([4.0473]), time : 399.6180810928345, lr:1.5625e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 60: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.15]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1675]), val loss is tensor([6.3133]), test loss is tensor([4.2974]), time : 398.9371461868286, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 61: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.143]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1666]), val loss is tensor([6.3474]), test loss is tensor([4.1396]), time : 398.02886271476746, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 62: 100%|██████████| 49/49 [05:46<00:00,  7.06s/it, loss=0.138]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1682]), val loss is tensor([6.3124]), test loss is tensor([4.2039]), time : 401.0844750404358, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 63: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.152]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1674]), val loss is tensor([6.3491]), test loss is tensor([4.1218]), time : 398.51811838150024, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 64: 100%|██████████| 49/49 [05:43<00:00,  7.02s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1655]), val loss is tensor([6.3636]), test loss is tensor([4.0975]), time : 398.73956274986267, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 65: 100%|██████████| 49/49 [05:45<00:00,  7.04s/it, loss=0.149]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1663]), val loss is tensor([6.3982]), test loss is tensor([4.0685]), time : 399.8442895412445, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 66: 100%|██████████| 49/49 [05:45<00:00,  7.04s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1684]), val loss is tensor([6.3253]), test loss is tensor([4.0476]), time : 399.8942880630493, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 67: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.143]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1666]), val loss is tensor([6.3404]), test loss is tensor([4.1211]), time : 399.9609293937683, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 68: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.13]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1676]), val loss is tensor([6.4053]), test loss is tensor([4.2186]), time : 401.29988956451416, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 69: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.13]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1663]), val loss is tensor([6.3689]), test loss is tensor([4.0671]), time : 399.24451065063477, lr:7.8125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 70: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.139]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1679]), val loss is tensor([6.3703]), test loss is tensor([4.1304]), time : 399.3109178543091, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 71: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.118]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1655]), val loss is tensor([6.3209]), test loss is tensor([4.1917]), time : 402.340318441391, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 72: 100%|██████████| 49/49 [05:43<00:00,  7.02s/it, loss=0.122]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1669]), val loss is tensor([6.3196]), test loss is tensor([4.1111]), time : 399.4993100166321, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 73: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.128]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1684]), val loss is tensor([6.3175]), test loss is tensor([4.3455]), time : 398.86384081840515, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 74: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.138]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1653]), val loss is tensor([6.3115]), test loss is tensor([4.2852]), time : 399.18833684921265, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 75: 100%|██████████| 49/49 [05:46<00:00,  7.07s/it, loss=0.14]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1660]), val loss is tensor([6.3247]), test loss is tensor([4.2539]), time : 401.90954780578613, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 76: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.141]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1661]), val loss is tensor([6.3158]), test loss is tensor([4.2454]), time : 399.88493037223816, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 77: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.134]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1659]), val loss is tensor([6.3197]), test loss is tensor([4.2080]), time : 399.8862364292145, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 78: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.152]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1659]), val loss is tensor([6.3374]), test loss is tensor([4.2573]), time : 399.0567317008972, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 79: 100%|██████████| 49/49 [05:43<00:00,  7.00s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1676]), val loss is tensor([6.3143]), test loss is tensor([4.2197]), time : 399.12297224998474, lr:3.90625e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 80: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.12]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1663]), val loss is tensor([6.3211]), test loss is tensor([4.1723]), time : 400.06136298179626, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 81: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1654]), val loss is tensor([6.3162]), test loss is tensor([4.2254]), time : 398.89978861808777, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 82: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.147]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1645]), val loss is tensor([6.3249]), test loss is tensor([4.1887]), time : 401.33099722862244, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 83: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1641]), val loss is tensor([6.3343]), test loss is tensor([4.2067]), time : 401.50519490242004, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 84: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.129]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1649]), val loss is tensor([6.3215]), test loss is tensor([4.2146]), time : 399.80571484565735, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 85: 100%|██████████| 49/49 [05:43<00:00,  7.01s/it, loss=0.159]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1648]), val loss is tensor([6.3066]), test loss is tensor([4.2366]), time : 399.98610734939575, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 86: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1669]), val loss is tensor([6.3240]), test loss is tensor([4.2139]), time : 400.8823857307434, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 87: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.134]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1656]), val loss is tensor([6.3167]), test loss is tensor([4.2406]), time : 399.4297640323639, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 88: 100%|██████████| 49/49 [05:44<00:00,  7.02s/it, loss=0.145]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1651]), val loss is tensor([6.3068]), test loss is tensor([4.2200]), time : 398.17767095565796, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 89: 100%|██████████| 49/49 [05:45<00:00,  7.06s/it, loss=0.138]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1651]), val loss is tensor([6.3241]), test loss is tensor([4.1892]), time : 402.4414412975311, lr:1.953125e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 90: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.143]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1672]), val loss is tensor([6.3259]), test loss is tensor([4.1143]), time : 400.7096734046936, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 91: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.136]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1668]), val loss is tensor([6.3219]), test loss is tensor([4.1860]), time : 400.54947662353516, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 92: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.131]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1639]), val loss is tensor([6.3147]), test loss is tensor([4.1169]), time : 402.4234576225281, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 93: 100%|██████████| 49/49 [05:45<00:00,  7.05s/it, loss=0.144]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1659]), val loss is tensor([6.2995]), test loss is tensor([4.2190]), time : 400.3465642929077, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 94: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.14]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1672]), val loss is tensor([6.3062]), test loss is tensor([4.2225]), time : 400.3692226409912, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 95: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.14]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1670]), val loss is tensor([6.3122]), test loss is tensor([4.1941]), time : 399.5193269252777, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 96: 100%|██████████| 49/49 [05:46<00:00,  7.07s/it, loss=0.119]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1678]), val loss is tensor([6.3312]), test loss is tensor([4.1699]), time : 401.082070350647, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 97: 100%|██████████| 49/49 [05:44<00:00,  7.04s/it, loss=0.124]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1644]), val loss is tensor([6.3162]), test loss is tensor([4.2106]), time : 400.0932800769806, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 98: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.143]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1672]), val loss is tensor([6.3184]), test loss is tensor([4.1836]), time : 398.6396176815033, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 99: 100%|██████████| 49/49 [05:44<00:00,  7.03s/it, loss=0.122]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1667]), val loss is tensor([6.3303]), test loss is tensor([4.1724]), time : 399.8043622970581, lr:9.765625e-07\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 100: 100%|██████████| 49/49 [05:45<00:00,  7.04s/it, loss=0.137]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([200.])\n",
            "training loss is tensor([0.1668]), val loss is tensor([6.3083]), test loss is tensor([4.2188]), time : 400.3767921924591, lr:4.8828125e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JWDtv0837Xy"
      },
      "source": [
        "device = xm.xla_device()"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeG9mU305AoU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3f1d5b-2602-4529-ba88-aaa3fbb0abd5"
      },
      "source": [
        "x = Tensor(torch.rand(32, 1024, 257)).to(device)\n",
        "x"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1206, 0.9304, 0.3156,  ..., 0.8086, 0.3009, 0.4681],\n",
              "         [0.8815, 0.6837, 0.3707,  ..., 0.7815, 0.9010, 0.3290],\n",
              "         [0.6040, 0.7827, 0.2778,  ..., 0.4897, 0.7872, 0.4663],\n",
              "         ...,\n",
              "         [0.3643, 0.3553, 0.6919,  ..., 0.1567, 0.4761, 0.7770],\n",
              "         [0.4954, 0.3765, 0.5860,  ..., 0.8771, 0.0902, 0.2280],\n",
              "         [0.4037, 0.6879, 0.3242,  ..., 0.8886, 0.6708, 0.8253]],\n",
              "\n",
              "        [[0.2157, 0.7032, 0.0611,  ..., 0.9114, 0.1399, 0.3812],\n",
              "         [0.1914, 0.0599, 0.6663,  ..., 0.8048, 0.0702, 0.2602],\n",
              "         [0.1016, 0.3417, 0.7902,  ..., 0.3945, 0.4644, 0.5231],\n",
              "         ...,\n",
              "         [0.6492, 0.8353, 0.7832,  ..., 0.7838, 0.9490, 0.7168],\n",
              "         [0.0445, 0.4788, 0.8923,  ..., 0.4467, 0.1360, 0.9395],\n",
              "         [0.5989, 0.2481, 0.9290,  ..., 0.4367, 0.8843, 0.9634]],\n",
              "\n",
              "        [[0.1666, 0.6024, 0.8801,  ..., 0.6033, 0.0741, 0.6264],\n",
              "         [0.8297, 0.5550, 0.9606,  ..., 0.7596, 0.0869, 0.9806],\n",
              "         [0.6259, 0.5252, 0.0628,  ..., 0.0047, 0.9107, 0.8001],\n",
              "         ...,\n",
              "         [0.3808, 0.9273, 0.6612,  ..., 0.2522, 0.8813, 0.4394],\n",
              "         [0.7634, 0.0420, 0.3055,  ..., 0.9683, 0.7610, 0.1207],\n",
              "         [0.4130, 0.2716, 0.5515,  ..., 0.9493, 0.0887, 0.0695]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.0572, 0.8548, 0.6347,  ..., 0.4161, 0.8735, 0.3335],\n",
              "         [0.5655, 0.6611, 0.4331,  ..., 0.4388, 0.1309, 0.0767],\n",
              "         [0.8438, 0.7054, 0.5498,  ..., 0.6072, 0.0650, 0.7326],\n",
              "         ...,\n",
              "         [0.6256, 0.4596, 0.4522,  ..., 0.4878, 0.7522, 0.3422],\n",
              "         [0.8308, 0.7488, 0.2813,  ..., 0.1954, 0.6842, 0.0232],\n",
              "         [0.4026, 0.9745, 0.9145,  ..., 0.8097, 0.6173, 0.4882]],\n",
              "\n",
              "        [[0.3839, 0.1650, 0.2727,  ..., 0.1571, 0.5209, 0.1119],\n",
              "         [0.1840, 0.5265, 0.8323,  ..., 0.2330, 0.0363, 0.4392],\n",
              "         [0.2936, 0.9958, 0.1082,  ..., 0.0306, 0.6702, 0.9346],\n",
              "         ...,\n",
              "         [0.0714, 0.6951, 0.0662,  ..., 0.9672, 0.5078, 0.8757],\n",
              "         [0.3966, 0.8550, 0.1886,  ..., 0.5084, 0.3597, 0.4255],\n",
              "         [0.4304, 0.1870, 0.6068,  ..., 0.9849, 0.4928, 0.5260]],\n",
              "\n",
              "        [[0.4275, 0.8011, 0.3547,  ..., 0.1910, 0.2405, 0.2967],\n",
              "         [0.8001, 0.5186, 0.3337,  ..., 0.3418, 0.8000, 0.2985],\n",
              "         [0.0826, 0.8944, 0.6446,  ..., 0.1448, 0.2961, 0.2599],\n",
              "         ...,\n",
              "         [0.6950, 0.2197, 0.6537,  ..., 0.9449, 0.4061, 0.8260],\n",
              "         [0.8330, 0.1640, 0.8917,  ..., 0.9505, 0.9909, 0.4447],\n",
              "         [0.1786, 0.2175, 0.4921,  ..., 0.5089, 0.4732, 0.8669]]],\n",
              "       device='xla:1')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRYFcJcuN865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b771cb-c44b-4fbb-9604-5ffd6862bb98"
      },
      "source": [
        "m = nn.AdaptiveAvgPool1d(1)\n",
        "input = torch.randn(1, 64, 8)\n",
        "output = m(input)\n",
        "output.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiC364ummwTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f01fea-74ef-4227-9101-688a69d60204"
      },
      "source": [
        "%tb"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No traceback available to show.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWpoaeu5o_S3"
      },
      "source": [
        "#baseline 5.40 -> 4.42, resnet 50, 16, 3.82, resnet 50, 32, 4.22, se-resnet 4.51"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8F8NhLVIOqt"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}